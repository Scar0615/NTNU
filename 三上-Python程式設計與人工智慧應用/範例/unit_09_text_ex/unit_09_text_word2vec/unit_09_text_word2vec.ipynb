{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1 align=\"center\">word2vec 使用入門（運用維基百科文本作為訓練語料）</h1>\n",
    "<hr>\n",
    "<h3>1. 取得維基百科語料（目前訓練使用 2021-06-10 版本，建議用最新版）</h3>\n",
    "<p><a href=\"https://dumps.wikimedia.org/zhwiki/\">https://dumps.wikimedia.org/zhwiki/</a></p>\n",
    "<p>\n",
    "要下載的是以 <strong style=\"color:red\">pages-articles.xml.bz2</strong> 結尾的備份，\n",
    "而不是以 pages-articles-multistream.xml.bz2 結尾的備份，\n",
    "否則會在清理上出現一些異常，無法正常解析文章。\n",
    "</p>\n",
    "<p>壓縮檔案超過 2G，解壓所後超過 9G（2021-06-10）</p>\n",
    "<hr>\n",
    "<p>\n",
    "這個專案的資料檔都非常大，避免 Dropbox 同步困難，若以 windows 作為工作平台，建議所有資料檔集中在本機，如：\n",
    "<strong style=\"color:magenta\">c:/python/wiki</strong> 資料夾下。\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h3>2. 安裝 gensim 工具模組（含 word2vec）</h3>\n",
    "<pre>\n",
    "<span style=\"color:orange\">conda install gensim</span>\n",
    "或\n",
    "<span style=\"color:orange\">pip3 install gensim</span>\n",
    "</pre>\n",
    "<pre>\n",
    "注意：\n",
    "<span style=\"color:red\">建議重新建立新的虛擬環境，安裝最新版本 gensim（目前 conda 最新是 4.0.1 版）</span>，\n",
    "若是舊環境增加模組安裝時，需注意版本問題，\n",
    "gensim 更新至 3.8.0 版以上測試正常（之前的舊版測試時發現有錯誤訊息），\n",
    "連帶 smart_open 與 numpy 也都需要更新，\n",
    "smart_open 更新為 1.8.4 版，\n",
    "numpy 更新為 1.17.2 版\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0.1\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "print(gensim.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h3>3. 將 XML 去除標籤輸出成 TXT 純文字檔（耗時約一個小時）</h3>\n",
    "<pre>\n",
    "讀取 wiki XML 文件檔（目前是採用 <strong style=\"color:red\">zhwiki-20210601-pages-articles.xml.bz2</strong>），\n",
    "刪除 XML 標籤與英文文數字與符號（非中文）的所有文字，\n",
    "輸出成純文字檔 <strong style=\"color:red\">wiki_texts.txt</strong>（約 1.2G）</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將 XML 去除標籤輸出成 TXT 純文字檔\n",
    "\n",
    "import gensim\n",
    "import re\n",
    "\n",
    "# 定義函數：去除非中文的文字與符號（只保留中文字）\n",
    "\n",
    "def remove_punctuation(line):\n",
    "    rule = re.compile(r'[^\\u4e00-\\u9fa5|\\s]')\n",
    "    line = rule.sub('', line)\n",
    "    return line\n",
    "\n",
    "# 定義函數：去除多餘的空白符號（只留下一個空白當作間隔）\n",
    "\n",
    "def remove_redundant_space(line):\n",
    "    line = re.sub(' +', ' ', line)\n",
    "    return line\n",
    "\n",
    "# 下載的 wiki 語料檔\n",
    "# wiki_file = 'c:/python/wiki/zhwiki-20190901-pages-articles.xml.bz2'\n",
    "wiki_file = 'c:/python/wiki/zhwiki-20210601-pages-articles.xml.bz2'\n",
    "\n",
    "with open('c:/python/wiki/wiki_texts.txt', 'w', encoding='utf8') as fp:\n",
    "    # 利用 gensim 載入\n",
    "    # wiki = gensim.corpora.WikiCorpus(wiki_file, lemmatize=False, dictionary={}) # 舊版，新版不支援 lemmatize\n",
    "    wiki = gensim.corpora.WikiCorpus(wiki_file, dictionary={})\n",
    "    # 取出文字部分（原本是 XML 格式，包含很多標籤）\n",
    "    for text in wiki.get_texts():\n",
    "        # print(text)\n",
    "        # text 是一篇文章，表示成字串串列（List）\n",
    "        # text 中的字串連接合併成長字串，以空白字元作為間隔\n",
    "        s = ' '.join(text)\n",
    "        # 僅保留中文\n",
    "        t = remove_punctuation(s)\n",
    "        # 只留下一個空白當作間隔\n",
    "        u = remove_redundant_space(t)\n",
    "        # 每篇文章一個換行作為間隔，寫入輸出檔案\n",
    "        fp.write(u + '\\n')\n",
    "\n",
    "fp.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h3>4. 利用 OpenCC 將 TXT 純文字內容都轉換為繁體（必要時，耗時超過一個小時）</h3>\n",
    "<pre>\n",
    "在 Linux 平台安裝 opencc：\n",
    "<span style=\"color:orange\">apt install opencc</span>\n",
    "執行：\n",
    "<span style=\"color:orange\">opencc -i wiki_texts.txt -o wiki_zh_tw.txt -c s2tw.json</span>\n",
    "（在 Linux 處理速度快很多）\n",
    "最後，輸出成繁體的 <strong style=\"color:red\">wiki_zh_tw.txt</strong> 文字檔\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h3>也可以安裝 python 的 opencc 工具模組來進行繁簡轉換</h3>\n",
    "<pre>\n",
    "opencc 是繁簡轉換工具模組\n",
    "安裝：（目前 conda 不提供此模組）\n",
    "<span style=\"color:orange\">pip3 install opencc-python-reimplemented</span>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 運用 python 的 opencc 工具模組執行繁體轉換\n",
    "\n",
    "from opencc import OpenCC\n",
    "\n",
    "openCC = OpenCC('s2t')\n",
    "\n",
    "with open('c:/python/wiki/wiki_texts.txt', 'r', encoding='utf-8') as fp:\n",
    "    s = fp.read()\n",
    "fp.close()\n",
    "\n",
    "t = openCC.convert(s)\n",
    "\n",
    "with open('c:/python/wiki/wiki_zh_tw.txt', 'w', encoding='utf-8') as fp:\n",
    "    fp.write(t)\n",
    "fp.close()\n",
    "\n",
    "print('Conversion Complete!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h3>5. 利用 jieba 工具模組進行斷詞（非常耗時的計算）</h3>\n",
    "<pre>\n",
    "安裝：\n",
    "<span style=\"color:orange\">conda install -c conda-forge jieba</span>\n",
    "或\n",
    "<span style=\"color:orange\">pip3 install jieba</span>\n",
    "最後，輸出成分詞（斷詞）後的 <strong style=\"color:red\">wiki_seg.txt</strong> 文字檔\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 利用 jieba 工具模組進行斷詞\n",
    "#\n",
    "# 目前沒有設定自用辭典（user diectionary）\n",
    "# jieba.load_userdict(file_name)\n",
    "# 特定專有術語應該放入此辭典中，如：CIS 180 image words\n",
    "# 目前也沒有設定 stopwords\n",
    "# 若除去英文（非中文）字元後，至少還應該除去全形標點符號\n",
    "#\n",
    "\n",
    "import jieba\n",
    "import logging\n",
    "\n",
    "def main():\n",
    "\n",
    "    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "    \n",
    "    # 支援繁體文句分詞的專用辭典\n",
    "    jieba.set_dictionary('dict.txt.big')\n",
    "    # 載入額外的使用者辭典\n",
    "    # jieba.load_userdict('ccom223_iwdb.txt')\n",
    "\n",
    "    # 輸出分詞結果\n",
    "    ofile = open('c:/python/wiki/wiki_seg.txt', 'w', encoding='utf-8')\n",
    "\n",
    "    n = 0\n",
    "    \n",
    "    with open('c:/python/wiki/wiki_zh_tw.txt' ,'r', encoding='utf-8') as ifile:\n",
    "        for line in ifile:\n",
    "            words = jieba.cut(line, cut_all=False)\n",
    "            for w in words:\n",
    "                ofile.write(w + ' ')\n",
    "                n = n + 1\n",
    "                if (n % 1000000 == 0):\n",
    "                    # print('已完成前 %d 行的斷詞' % n)\n",
    "                    logging.info('已完成前 %d 行的斷詞' % n)\n",
    "\n",
    "    ifile.close()\n",
    "    ofile.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h3>6. word2vec 訓練（稍微耗時的計算）</h3>\n",
    "<pre>\n",
    "vector 長度：<strong style=\"color:red\">250-400</strong>\n",
    "最後，輸出 word2vec model 檔：<strong style=\"color:red\">wiki.model.bin</strong>\n",
    "</pre>\n",
    "<pre>\n",
    "<span style=\"color:orange\">model = word2vec.Word2Vec(sentences, size=400, iter=10, min_count=1)</span>\n",
    "size=400 # word vector 維度（視需要而定，一般中文語料 250-400）\n",
    "iter=10 # 訓練回合數（epoch，預設 5）\n",
    "min_count=1 # 出現詞頻數（超過這個次數的詞就會被列入辭典）\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練 word2vec model\n",
    "#\n",
    "\n",
    "from gensim.models import word2vec\n",
    "import logging\n",
    "\n",
    "def main():\n",
    "    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "    sentences = word2vec.Text8Corpus('c:/python/wiki/wiki_seg.txt')\n",
    "    model = word2vec.Word2Vec(sentences, size=400)\n",
    "    # Save our model.\n",
    "    model.save('c:/python/wiki/wiki.model.bin')\n",
    "    # To load a model.\n",
    "    # model = word2vec.Word2Vec.load('c:/python/wiki/wiki.model.bin')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h3>7. 測試（相似詞測試、相似度測試、詞向量）</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h3>載入 word2vec model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 載入 word2vec model\n",
    "\n",
    "import gensim\n",
    "\n",
    "print(gensim.__version__)\n",
    "\n",
    "model = gensim.models.Word2Vec.load('c:/python/wiki/wiki.model.bin')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h3>相似詞測試</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 相似詞測試\n",
    "\n",
    "q = '快樂'\n",
    "\n",
    "try:\n",
    "    lst = model.wv.most_similar(q)\n",
    "except:\n",
    "    print('No %s in corpus!' % q)\n",
    "    lst = []\n",
    "\n",
    "for i in lst:\n",
    "    t, w = i\n",
    "    print('%20.16f: %s' % (w, t))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h3>相似度測試</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 相似度測試\n",
    "\n",
    "q1 = '快樂'\n",
    "q2 = '高興'\n",
    "\n",
    "try:\n",
    "    s = model.wv.similarity(q1, q2)\n",
    "except:\n",
    "    s = 0\n",
    "    print('無從測試!')\n",
    "\n",
    "print(s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h3>詞向量與餘弦距離計算</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 詞向量與餘弦距離計算\n",
    "\n",
    "import gensim\n",
    "import math\n",
    "\n",
    "q1 = '巴洛克'\n",
    "q2 = '古典'\n",
    "\n",
    "sm = model.wv.similarity(q1, q2)\n",
    "\n",
    "# KeyedVectors Instance gets stored\n",
    "# v1 = model.wv.word_vec(q1)\n",
    "# v2 = model.wv.word_vec(q2)\n",
    "v1 = model.wv.get_vector(q1)\n",
    "v2 = model.wv.get_vector(q2)\n",
    "\n",
    "print(q1, '=')\n",
    "print(v1)\n",
    "print(q2, '=')\n",
    "print(v2)\n",
    "\n",
    "# Cosine value of word vectors\n",
    "s0, s1, s2 = 0.0, 0.0, 0.0\n",
    "for i in range(len(v1)):\n",
    "    s0 = s0 + (v1[i] * v2[i])\n",
    "    s1 = s1 + (v1[i] * v1[i])\n",
    "    s2 = s2 + (v2[i] * v2[i])\n",
    "s1 = math.sqrt(s1)\n",
    "s2 = math.sqrt(s2)\n",
    "cs = s0 / (s1 * s2)\n",
    "\n",
    "print('similarity =', sm)\n",
    "print('    cosine =', cs)\n",
    "\n",
    "# for i in range(len(v1)):\n",
    "#     t = '%3d. %8.4f %8.4f' % (i, v1[i], v2[i])\n",
    "#     print(t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<hr>\n",
    "<h3>以下為暫時性測試</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenCC 測試\n",
    "\n",
    "from opencc import OpenCC\n",
    "\n",
    "# convert from Simplified Chinese to Traditional Chinese\n",
    "openCC = OpenCC('s2t')\n",
    "\n",
    "# can also set conversion by calling set_conversion\n",
    "# openCC.set_conversion('s2tw')\n",
    "\n",
    "chs= '开放中文转换'\n",
    "cht= openCC.convert(chs)\n",
    "\n",
    "print(cht)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 去除所有半角全角符号，只留字母、数字、中文。\n",
    "\n",
    "import re\n",
    "\n",
    "def remove_punctuation(line):\n",
    "    rule = re.compile(r'[^\\u4e00-\\u9fa5|\\s]')\n",
    "    line = rule.sub('', line)\n",
    "    return line\n",
    "\n",
    "def remove_redundant_space(line):\n",
    "    line = re.sub(' +', ' ', line)\n",
    "    return line\n",
    "\n",
    "s = '开放中文转换 abc, XyZ#$%， 塵土    123 飛揚'\n",
    "\n",
    "t = remove_punctuation(s)\n",
    "u = remove_redundant_space(t)\n",
    "\n",
    "print(u)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = '中文字   空白   刪除 ！'\n",
    "t = re.sub(' +', ' ', s)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
