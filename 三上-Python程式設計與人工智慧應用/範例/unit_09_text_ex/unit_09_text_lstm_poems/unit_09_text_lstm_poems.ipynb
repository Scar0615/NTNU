{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AUY8h8T-hQFu"
   },
   "source": [
    "<h1 align=\"center\">LSTM 唐詩生成 Trained by 全唐詩</h1>\n",
    "<hr>\n",
    "<p><a href=\"全唐詩.txt\">全唐詩.txt</a></p>\n",
    "<hr>\n",
    "<pre>\n",
    "每次輸入一個 word vector 的數值（input_size=32）\n",
    "連續輸入五個 word vectors（seq_len=5）\n",
    "預測下一個 word vector（output_size=32）\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h3>載入所有 Word Vectors</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 載入所有 Word Vectors\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('word_vec.pkl', 'rb') as fp:\n",
    "    word_vec = pickle.load(fp)\n",
    "fp.close()\n",
    "\n",
    "i = 0\n",
    "for c in word_vec:\n",
    "    print(c, word_vec[c])\n",
    "    i = i + 1\n",
    "    if (i == 10):\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h3>載入全唐詩文本</h3>\n",
    "<p style=\"color:red\">全唐詩（單字）.txt</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 載入全唐詩文本\n",
    "\n",
    "with open('全唐詩（單字）.txt', 'r', encoding='utf-8') as fp:\n",
    "    txt = fp.read()\n",
    "fp.close()\n",
    "\n",
    "n = 0\n",
    "for c in txt:\n",
    "    if (c != ' '):\n",
    "        n = n + 1\n",
    "\n",
    "print('總字數：', n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h3>產生 Corpus of Word Vectors</h3>\n",
    "<pre>\n",
    "total sequence length = 2,563,538\n",
    "input_tensor shape = (batch, seq_len, input_size)\n",
    "batch = 128\n",
    "seq_len = 5\n",
    "input_size = 32 (dim of input feature vector)\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 產生 Corpus of Word Vectors\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "corpus = np.zeros([n,input_size], dtype=np.float32)\n",
    "\n",
    "i = 0\n",
    "for c in txt:\n",
    "    if (c != ' '):\n",
    "        corpus[i] = word_vec[c]\n",
    "        i = i + 1\n",
    "\n",
    "print(corpus.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.randint(0, n-1)\n",
    "print(idx)\n",
    "print(corpus[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h3>LSTM Parameters</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Parameters\n",
    "\n",
    "input_size = 32 # 輸入特徵數目\n",
    "batch = 1024 # 128 # 訓練樣本數\n",
    "seq_len = 5 # 連續送幾個信號後輸出預測\n",
    "hidden_size = 1024 # 隱藏層細胞數\n",
    "num_layers = 3 # 隱藏層層數\n",
    "output_size = 32 # 輸出數值數目\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h3>先準備好第一組 batch 訓練樣本（PyTorch Tensor）</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 先準備好第一組 batch 訓練樣本（PyTorch Tensor）\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "x = torch.zeros([batch,seq_len,input_size])\n",
    "y = torch.zeros([batch,output_size])\n",
    "\n",
    "for i in range(batch):\n",
    "    for j in range(seq_len):\n",
    "        x[i,j,:] = torch.from_numpy(corpus[i+j])\n",
    "    y[i,:] = torch.from_numpy(corpus[i+seq_len])\n",
    "\n",
    "print(x.shape)\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h3>定義與建立 LSTM 模型</h3>\n",
    "<p style=\"color:red\">batch_first=True</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定義與建立 LSTM 模型\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyLSTM(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(MyLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size = input_size,\n",
    "            hidden_size = hidden_size,\n",
    "            num_layers = num_layers,\n",
    "            batch_first = True\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "        self.h0 = torch.randn(num_layers, batch, hidden_size)\n",
    "        self.c0 = torch.randn(num_layers, batch, hidden_size)\n",
    "    def forward(self, x):\n",
    "        h_out, (hn,cn) = self.lstm(x, (self.h0,self.c0))\n",
    "        out = self.fc(h_out)\n",
    "        return out\n",
    "\n",
    "lstm = MyLSTM(input_size)\n",
    "\n",
    "print(lstm)\n",
    "\n",
    "# params = list(lstm.parameters())\n",
    "# print(params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h3>單一 batch 樣本 LSTM Training</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 單一 batch 樣本 LSTM Training\n",
    "\n",
    "# Loss function\n",
    "loss_func = nn.MSELoss()\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=0.001)\n",
    "\n",
    "EPOCH = 100\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    optimizer.zero_grad()\n",
    "    # shuffling\n",
    "    permute = torch.randperm(x.size()[0])\n",
    "    xi = x[permute]\n",
    "    yi = y[permute]\n",
    "    # feed forward\n",
    "    output = lstm(xi)\n",
    "    # print(output[:,-1,:].shape)\n",
    "    # print(yi.shape)\n",
    "    # evaluating loss\n",
    "    loss = loss_func(output[:,-1,:], yi)\n",
    "    # display loss\n",
    "    if (epoch % 1 == 0):\n",
    "        print('epoch = %5d, loss = %16.12f' % (epoch, loss.item()))\n",
    "    # feed backward\n",
    "    loss.backward()\n",
    "    # update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "torch.save(lstm.state_dict(), 'lstm_poems.model')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h3>整個文本逐 batch 樣本 LSTM Training</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 整個文本逐 batch 樣本 LSTM Training\n",
    "\n",
    "x = torch.zeros([batch,seq_len,input_size])\n",
    "y = torch.zeros([batch,output_size])\n",
    "\n",
    "# Loss function\n",
    "loss_func = nn.MSELoss()\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=0.001)\n",
    "\n",
    "EPOCH = 100\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    loss_total = 0\n",
    "    bn = 0\n",
    "    for base in range(0, n-batch+1-seq_len, batch):\n",
    "        for i in range(batch):\n",
    "            for j in range(seq_len):\n",
    "                x[i,j,:] = torch.from_numpy(corpus[base+i+j])\n",
    "            y[i,:] = torch.from_numpy(corpus[base+i+seq_len])\n",
    "        optimizer.zero_grad()\n",
    "        # shuffling\n",
    "        permute = torch.randperm(x.size()[0])\n",
    "        xi = x[permute]\n",
    "        yi = y[permute]\n",
    "        # feed forward\n",
    "        output = lstm(xi)\n",
    "        # print(output[:,-1,:].shape)\n",
    "        # print(yi.shape)\n",
    "        # evaluating loss\n",
    "        loss = loss_func(output[:,-1,:], yi)\n",
    "        loss_total = loss_total + loss.item()\n",
    "        bn = bn + 1\n",
    "        # feed backward\n",
    "        loss.backward()\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        # display status\n",
    "        if (bn % 10 == 0):\n",
    "            print('base = %8d, loss = %16.12f' % (base, loss.item()))\n",
    "    # display loss\n",
    "    loss = loss_total / bn\n",
    "    if (epoch % 1 == 0):\n",
    "        print('epoch = %5d, loss = %16.12f' % (epoch, loss))\n",
    "\n",
    "torch.save(lstm.state_dict(), 'lstm_poems.model')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h3 style=\"color:orange\">LSTM 全唐詩文本訓練（整合版，含 GPU 加速）</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM 全唐詩文本訓練（整合版）\n",
    "\n",
    "# LSTM Parameters\n",
    "\n",
    "input_size = 32 # 輸入特徵數目\n",
    "batch = 1024 # 128 # 訓練樣本數\n",
    "seq_len = 5 # 連續送幾個信號後輸出預測\n",
    "hidden_size = 1024 # 隱藏層細胞數\n",
    "num_layers = 3 # 隱藏層層數\n",
    "output_size = 32 # 輸出數值數目\n",
    "\n",
    "# 載入所有 Word Vectors\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('word_vec.pkl', 'rb') as fp:\n",
    "    word_vec = pickle.load(fp)\n",
    "fp.close()\n",
    "\n",
    "# 載入全唐詩文本\n",
    "\n",
    "with open('全唐詩（單字）.txt', 'r', encoding='utf-8') as fp:\n",
    "    txt = fp.read()\n",
    "fp.close()\n",
    "\n",
    "n = 0\n",
    "for c in txt:\n",
    "    if (c != ' '):\n",
    "        n = n + 1\n",
    "print('總字數：', n)\n",
    "\n",
    "# 產生 Corpus of Word Vectors\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "corpus = np.zeros([n,input_size], dtype=np.float32)\n",
    "\n",
    "i = 0\n",
    "for c in txt:\n",
    "    if (c != ' '):\n",
    "        corpus[i] = word_vec[c]\n",
    "        i = i + 1\n",
    "\n",
    "# print(corpus.shape)\n",
    "\n",
    "# GPU 偵測\n",
    "\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)\n",
    "\n",
    "# 定義與建立 LSTM 模型\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyLSTM(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(MyLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size = input_size,\n",
    "            hidden_size = hidden_size,\n",
    "            num_layers = num_layers,\n",
    "            batch_first = True\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "        self.h0 = torch.randn(num_layers, batch, hidden_size).to(device)\n",
    "        self.c0 = torch.randn(num_layers, batch, hidden_size).to(device)\n",
    "    def forward(self, x):\n",
    "        h_out, (hn,cn) = self.lstm(x, (self.h0,self.c0))\n",
    "        out = self.fc(h_out)\n",
    "        return out\n",
    "\n",
    "lstm = MyLSTM(input_size).to(device)\n",
    "\n",
    "# print(lstm)\n",
    "\n",
    "# 整個文本逐 batch 樣本 LSTM Training\n",
    "\n",
    "# Loss function\n",
    "loss_func = nn.MSELoss()\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=0.001)\n",
    "\n",
    "EPOCH = 1000\n",
    "\n",
    "# LSTM Training\n",
    "\n",
    "x = torch.zeros([batch,seq_len,input_size]).to(device)\n",
    "y = torch.zeros([batch,output_size]).to(device)\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    loss_total = 0\n",
    "    bn = 0\n",
    "    for base in range(0, n-batch+1-seq_len, batch):\n",
    "        for i in range(batch):\n",
    "            for j in range(seq_len):\n",
    "                x[i,j,:] = torch.from_numpy(corpus[base+i+j])\n",
    "            y[i,:] = torch.from_numpy(corpus[base+i+seq_len])\n",
    "        optimizer.zero_grad()\n",
    "        # shuffling\n",
    "        permute = torch.randperm(x.size()[0])\n",
    "        xi = x[permute]\n",
    "        yi = y[permute]\n",
    "        # feed forward\n",
    "        output = lstm(xi)\n",
    "        # print(output[:,-1,:].shape)\n",
    "        # print(yi.shape)\n",
    "        # evaluating loss\n",
    "        loss = loss_func(output[:,-1,:], yi)\n",
    "        loss_total = loss_total + loss.item()\n",
    "        bn = bn + 1\n",
    "        # feed backward\n",
    "        loss.backward()\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        # display status\n",
    "        if (bn % 10 == 0):\n",
    "            print('base = %8d, loss = %16.12f' % (base, loss.item()))\n",
    "    # save model\n",
    "    if (epoch % 100 == 0):\n",
    "        torch.save(lstm.state_dict(), 'lstm_poems_%05d.model'%(epoch))\n",
    "    # display loss\n",
    "    loss = loss_total / bn\n",
    "    if (epoch % 1 == 0):\n",
    "        print('epoch = %5d, loss = %16.12f' % (epoch, loss))\n",
    "\n",
    "torch.save(lstm.state_dict(), 'lstm_poems.model')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h3>測試（第 04 組參數）</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 測試\n",
    "\n",
    "# LSTM Parameters\n",
    "\n",
    "input_size = 32 # 輸入特徵數目\n",
    "batch = 128 # 256 # 1024 # 訓練樣本數\n",
    "seq_len = 5 # 連續送幾個信號後輸出預測\n",
    "hidden_size = 1024 # 2048 # 256 # 隱藏層細胞數\n",
    "num_layers = 4 # 隱藏層層數\n",
    "output_size = 32 # 輸出數值數目\n",
    "\n",
    "# 定義 LSTM\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyLSTM(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(MyLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size = input_size,\n",
    "            hidden_size = hidden_size,\n",
    "            num_layers = num_layers,\n",
    "            batch_first = True\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "        self.h0 = torch.randn(num_layers, batch, hidden_size)\n",
    "        self.c0 = torch.randn(num_layers, batch, hidden_size)\n",
    "    def forward(self, x):\n",
    "        h_out, (hn,cn) = self.lstm(x, (self.h0,self.c0))\n",
    "        out = self.fc(h_out)\n",
    "        return out\n",
    "\n",
    "lstm = MyLSTM(input_size)\n",
    "\n",
    "# 載入 LSTM\n",
    "\n",
    "mfile = 'lstm_poems_00300.model'\n",
    "lstm.load_state_dict(torch.load(mfile, map_location='cpu'))\n",
    "lstm.eval()\n",
    "print('Load previous nn model completely!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h3>載入所有 Word Vectors</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 載入所有 Word Vectors\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('word_vec.pkl', 'rb') as fp:\n",
    "    word_vec = pickle.load(fp)\n",
    "fp.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h3>載入 word2vec model (char2vec)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 載入 word2vec model (char2vec)\n",
    "\n",
    "import gensim\n",
    "\n",
    "print(gensim.__version__)\n",
    "\n",
    "model = gensim.models.Word2Vec.load('poems.model.bin')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h3>提示靈感啟動</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提示靈感啟動\n",
    "\n",
    "s = '疑是地上霜'\n",
    "\n",
    "x = torch.zeros([batch,seq_len,input_size])\n",
    "\n",
    "for b in range(batch):\n",
    "    i = 0\n",
    "    for c in s:\n",
    "        x[b,i] = torch.from_numpy(word_vec[c])\n",
    "    i = i + 1\n",
    "\n",
    "print(x.shape)\n",
    "\n",
    "output = lstm(x)\n",
    "output = output[:,-1,:].detach()\n",
    "print(output.shape)\n",
    "v = output[0].numpy()\n",
    "print(v)\n",
    "\n",
    "p = model.wv.similar_by_vector(v, topn=1)\n",
    "print(p[0][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h3>五言絕句</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 五言絕句\n",
    "\n",
    "for i in range(20):\n",
    "    for b in range(batch):\n",
    "        for k in range(1, seq_len):\n",
    "            x[b,k-1] = x[b,k]\n",
    "        x[b, seq_len-1] = output[b]\n",
    "    output = lstm(x)\n",
    "    output = output[:,-1,:].detach()\n",
    "    # print(output.shape)\n",
    "    v = output[0].numpy()\n",
    "    p = model.wv.similar_by_vector(v, topn=1)\n",
    "    print(p[0][0], end='')\n",
    "    if (i % 5 == 4):\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "06 mlp_project_color_12.ipynb",
   "provenance": [
    {
     "file_id": "1vEtyTQvCdzNii0RG0MVwOIUEZ8ow3_b1",
     "timestamp": 1577695678501
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
