{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">中文資訊檢索（文件檢索）採用 TF-IDF 方法</h1>\n",
    "<h3 align=\"center\">Chinese Document Retrieval with TF-IDF</h3>\n",
    "<hr>\n",
    "<pre>\n",
    "doc: 文件資料夾（純文字，utf-8）\n",
    "qry: 查詢字串\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h3>詞法分析，工具函數定義（刪除非中文的所有文字與符號）</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 詞法分析，工具函數定義（刪除非中文的所有文字與符號）\n",
    "\n",
    "import re\n",
    "\n",
    "def remove_non_chinese(line):\n",
    "    # 消除英文文數字\n",
    "    rule = re.compile('[a-zA-Z0-9]')\n",
    "    line = rule.sub(' ', line)\n",
    "    # 消除特殊符號（含部分全形符號）\n",
    "    rule = re.compile('[’!\"#$%&\\'()*+,-./:;<=>?@，。?★、…【】《》？“”‘’！[\\\\]^_`{|}~\\s]+')\n",
    "    line = rule.sub(' ', line)\n",
    "    # 消除不可見字碼\n",
    "    rule = re.compile('[\\001\\002\\003\\004\\005\\006\\007\\x08\\x09\\x0a\\x0b\\x0c\\x0d\\x0e\\x0f\\x10\\x11\\x12\\x13\\x14\\x15\\x16\\x17\\x18\\x19\\x1a]+')\n",
    "    line = rule.sub(' ', line)\n",
    "    # 消除所有全形符號\n",
    "    rule = re.compile('[^\\u4e00-\\u9fa5]')\n",
    "    line = rule.sub(' ', line)\n",
    "    return line\n",
    "\n",
    "def remove_redundant_space(line):\n",
    "    line = re.sub(' +', ' ', line)\n",
    "    return line\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h3>詞法分析，載入 jieba 工具模組</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from C:\\Users\\trchou\\Dropbox\\000 teaching\\aisd10a\\jupyter\\unit_09_text\\unit_09_text_tfidf_info_retrieval\\dict.txt.big ...\n",
      "Loading model from cache C:\\Users\\trchou\\AppData\\Local\\Temp\\jieba.uba41133c6d6ca814317cbdbde7c4be49.cache\n",
      "Loading model cost 1.239 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "# 詞法分析，載入 jieba 模組\n",
    "\n",
    "import jieba\n",
    "\n",
    "# 有必要的話載入常用辭典\n",
    "jieba.set_dictionary('dict.txt.big')\n",
    "# 有必要的話載入專屬字典\n",
    "jieba.load_userdict('user.txt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h3>過濾無意義詞彙（stopwords，虛字與其它無意義詞彙）</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 過濾無意義詞彙（stopwords，虛字與其它無意義詞彙）\n",
    "\n",
    "stopwords = [ '之', '乎', '者', '也', '的' ]\n",
    "\n",
    "def remove_stopword(lst0):\n",
    "    lst = []\n",
    "    for x in lst0:\n",
    "        if x not in stopwords:\n",
    "            lst.append(x)\n",
    "    return lst\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h3>詞法分析，函數定義</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 詞法分析，函數定義\n",
    "\n",
    "def lexical_analyzer(txt):\n",
    "    txt = remove_non_chinese(txt)\n",
    "    txt = remove_redundant_space(txt)\n",
    "    res = jieba.cut(txt)\n",
    "    lst = [ x for x in res ]\n",
    "    lst = remove_stopword(lst)\n",
    "    doc = ' '.join(lst)\n",
    "    doc = re.sub(' +', ' ', doc)\n",
    "    return doc\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h3>整理 Corpus 資料格式</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc\\哈利波特01神密的魔法石.txt\n",
      "doc\\哈利波特02消失的密室.txt\n",
      "doc\\哈利波特03阿茲卡班的逃犯.txt\n",
      "doc\\哈利波特04火盃的考驗(上).txt\n",
      "doc\\哈利波特05火盃的考驗(下).txt\n",
      "doc\\哈利波特06鳳凰會的密令.txt\n",
      "doc\\哈利波特07混血王子的背叛.txt\n",
      "doc\\哈利波特08死神的聖物.txt\n"
     ]
    }
   ],
   "source": [
    "# 整理 Corpus 資料格式\n",
    "\n",
    "import glob\n",
    "import pickle\n",
    "\n",
    "files = glob.glob('doc/*.txt')\n",
    "\n",
    "corpus = dict()\n",
    "\n",
    "for f in files:\n",
    "    with open(f, 'r', encoding='utf-8') as fp:\n",
    "        txt = fp.read()\n",
    "    fp.close()\n",
    "    corpus[f] = { 'name':f, 'txt':txt, 'doc':None }\n",
    "\n",
    "for f in corpus:\n",
    "    print(f)\n",
    "    txt = corpus[f]['txt']\n",
    "    doc = lexical_analyzer(txt)\n",
    "    corpus[f]['doc'] = doc\n",
    "\n",
    "with open('corpus_harry_potter.pkl', 'wb') as fp:\n",
    "    pickle.dump(corpus, fp)\n",
    "fp.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h3>TF-IDF 文件檢索</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "查詢： 分類咒語\n",
      "關連度：0.0164944566，文章：doc\\哈利波特04火盃的考驗(上).txt\n",
      "關連度：0.0131317313，文章：doc\\哈利波特07混血王子的背叛.txt\n",
      "關連度：0.0101660171，文章：doc\\哈利波特06鳳凰會的密令.txt\n",
      "關連度：0.0097156026，文章：doc\\哈利波特02消失的密室.txt\n",
      "關連度：0.0091124249，文章：doc\\哈利波特05火盃的考驗(下).txt\n",
      "關連度：0.0089375430，文章：doc\\哈利波特01神密的魔法石.txt\n",
      "關連度：0.0086871125，文章：doc\\哈利波特08死神的聖物.txt\n",
      "關連度：0.0043297963，文章：doc\\哈利波特03阿茲卡班的逃犯.txt\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF 文件檢索\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "qry = '分類咒語'\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "def cosine_sim(text1, text2):\n",
    "    tfidf = vectorizer.fit_transform([text1, text2])\n",
    "    return ((tfidf * tfidf.T).A)[0,1]\n",
    "\n",
    "d = dict()\n",
    "for f in corpus:\n",
    "    text1 = lexical_analyzer(qry)\n",
    "    text2 = corpus[f]['doc']\n",
    "    c = cosine_sim(text1, text2)\n",
    "    d[f] = c\n",
    "\n",
    "# 字典排序（成為串列）\n",
    "lst = sorted(d.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# 顯示排序結果\n",
    "print('查詢：', qry)\n",
    "for f, c in lst:\n",
    "    t = '關連度：%12.10f，文章：%s' % (c, f)\n",
    "    print(t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h3 style=\"color:orange\">『朱自清散文』語料的資料檢索平台建立（整合版）</h3>\n",
    "<pre>\n",
    "語料來源：\n",
    "<a href=\"http://www.bwsk.net/mj/z/zhuziqing/index.html\">http://www.bwsk.net/mj/z/zhuziqing/index.html</a>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from C:\\Users\\trchou\\Dropbox\\000 teaching\\aisd10a\\jupyter\\unit_09_text\\unit_09_text_tfidf_info_retrieval\\dict.txt.big ...\n",
      "Loading model from cache C:\\Users\\trchou\\AppData\\Local\\Temp\\jieba.uba41133c6d6ca814317cbdbde7c4be49.cache\n",
      "Loading model cost 1.043 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "# 『朱自清散文』語料的資料檢索平台建立（整合版）\n",
    "\n",
    "# 詞法分析，工具函數定義（刪除非中文的所有文字與符號）\n",
    "\n",
    "import re\n",
    "\n",
    "def remove_non_chinese(line):\n",
    "    # 消除英文文數字\n",
    "    rule = re.compile('[a-zA-Z0-9]')\n",
    "    line = rule.sub(' ', line)\n",
    "    # 消除特殊符號（含部分全形符號）\n",
    "    rule = re.compile('[’!\"#$%&\\'()*+,-./:;<=>?@，。?★、…【】《》？“”‘’！[\\\\]^_`{|}~\\s]+')\n",
    "    line = rule.sub(' ', line)\n",
    "    # 消除不可見字碼\n",
    "    rule = re.compile('[\\001\\002\\003\\004\\005\\006\\007\\x08\\x09\\x0a\\x0b\\x0c\\x0d\\x0e\\x0f\\x10\\x11\\x12\\x13\\x14\\x15\\x16\\x17\\x18\\x19\\x1a]+')\n",
    "    line = rule.sub(' ', line)\n",
    "    # 消除所有全形符號\n",
    "    rule = re.compile('[^\\u4e00-\\u9fa5]')\n",
    "    line = rule.sub(' ', line)\n",
    "    return line\n",
    "\n",
    "def remove_redundant_space(line):\n",
    "    line = re.sub(' +', ' ', line)\n",
    "    return line\n",
    "\n",
    "# 詞法分析，載入 jieba 模組\n",
    "\n",
    "import jieba\n",
    "\n",
    "# 有必要的話載入常用辭典\n",
    "jieba.set_dictionary('dict.txt.big')\n",
    "# 有必要的話載入專屬字典\n",
    "jieba.load_userdict('user.txt')\n",
    "\n",
    "# 無意義詞彙過濾（stopwords，虛字與其它無意義詞彙）\n",
    "\n",
    "stopwords = [ '之', '乎', '者', '也', '的' ]\n",
    "\n",
    "def remove_stopword(lst0):\n",
    "    lst = []\n",
    "    for x in lst0:\n",
    "        if x not in stopwords:\n",
    "            lst.append(x)\n",
    "    return lst\n",
    "\n",
    "# 詞法分析，函數定義\n",
    "\n",
    "def lexical_analyzer(txt):\n",
    "    txt = remove_non_chinese(txt)\n",
    "    txt = remove_redundant_space(txt)\n",
    "    res = jieba.cut(txt)\n",
    "    lst = [ x for x in res ]\n",
    "    lst = remove_stopword(lst)\n",
    "    doc = ' '.join(lst)\n",
    "    doc = re.sub(' +', ' ', doc)\n",
    "    return doc\n",
    "\n",
    "# 整理 Corpus 資料格式\n",
    "\n",
    "import glob\n",
    "import pickle\n",
    "\n",
    "files = glob.glob('朱自清散文/*.txt')\n",
    "\n",
    "corpus = dict()\n",
    "\n",
    "for f in files:\n",
    "    with open(f, 'r', encoding='utf-8') as fp:\n",
    "        txt = fp.read()\n",
    "    fp.close()\n",
    "    corpus[f] = { 'name':f, 'txt':txt, 'doc':None }\n",
    "\n",
    "# 儲存語料庫\n",
    "\n",
    "for f in corpus:\n",
    "    txt = corpus[f]['txt']\n",
    "    doc = lexical_analyzer(txt)\n",
    "    corpus[f]['doc'] = doc\n",
    "\n",
    "with open('corpus_朱自清散文.pkl', 'wb') as fp:\n",
    "    pickle.dump(corpus, fp)\n",
    "fp.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h3 style=\"color:orange\">測試</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "查詢： 月台+橘子\n",
      "關連度：0.1473410365，文章：朱自清散文\\背影.txt\n",
      "關連度：0.0000000000，文章：朱自清散文\\《梅花》后記.txt\n",
      "關連度：0.0000000000，文章：朱自清散文\\一封信.txt\n",
      "關連度：0.0000000000，文章：朱自清散文\\儿女.txt\n",
      "關連度：0.0000000000，文章：朱自清散文\\匆匆.txt\n",
      "關連度：0.0000000000，文章：朱自清散文\\哀韋杰三君.txt\n",
      "關連度：0.0000000000，文章：朱自清散文\\女人.txt\n",
      "關連度：0.0000000000，文章：朱自清散文\\怀魏握青君.txt\n",
      "關連度：0.0000000000，文章：朱自清散文\\旅行雜記.txt\n",
      "關連度：0.0000000000，文章：朱自清散文\\槳聲燈影里的秦淮河.txt\n",
      "關連度：0.0000000000，文章：朱自清散文\\歌聲.txt\n",
      "關連度：0.0000000000，文章：朱自清散文\\海行雜記.txt\n",
      "關連度：0.0000000000，文章：朱自清散文\\溫州的蹤跡.txt\n",
      "關連度：0.0000000000，文章：朱自清散文\\白种人——上帝的驕子！.txt\n",
      "關連度：0.0000000000，文章：朱自清散文\\白采.txt\n",
      "關連度：0.0000000000，文章：朱自清散文\\航船中的文明.txt\n",
      "關連度：0.0000000000，文章：朱自清散文\\荷塘月色.txt\n",
      "關連度：0.0000000000，文章：朱自清散文\\說夢.txt\n",
      "關連度：0.0000000000，文章：朱自清散文\\阿河.txt\n",
      "關連度：0.0000000000，文章：朱自清散文\\飄零.txt\n"
     ]
    }
   ],
   "source": [
    "# 測試\n",
    "\n",
    "# 載入語料庫\n",
    "\n",
    "with open('corpus_朱自清散文.pkl', 'rb') as fp:\n",
    "    corpus = pickle.load(fp)\n",
    "fp.close()\n",
    "\n",
    "# TF-IDF 文件檢索\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "qry = '月台+橘子'\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "def cosine_sim(text1, text2):\n",
    "    tfidf = vectorizer.fit_transform([text1, text2])\n",
    "    return ((tfidf * tfidf.T).A)[0,1]\n",
    "\n",
    "d = dict()\n",
    "for f in corpus:\n",
    "    text1 = lexical_analyzer(qry)\n",
    "    text2 = corpus[f]['doc']\n",
    "    c = cosine_sim(text1, text2)\n",
    "    d[f] = c\n",
    "\n",
    "# 字典排序（成為串列）\n",
    "lst = sorted(d.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# 顯示排序結果\n",
    "print('查詢：', qry)\n",
    "for f, c in lst:\n",
    "    t = '關連度：%12.10f，文章：%s' % (c, f)\n",
    "    print(t)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
